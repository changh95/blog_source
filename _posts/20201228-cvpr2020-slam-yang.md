---
title: CVPR 2020 - SLAM 워크샵 Visual SLAM with Object and Plane 발표 노트 (Shichao Yang 발표)
date: 2020-12-28 20:38:19
mathjax: true
tags: 
- SLAM
- Visual-SLAM
- Deep Learning
- Deep SLAM
- CV
- 3D
- AR
- CubeSLAM
categories: 
- [SLAM]
excerpt: CVPR 2020 학회에서 Joint Workshop on Long-Term Visual Localization, Visual Odometry and Geometric and Learning-based SLAM 워크샵 중 Shichao Yang께서 발표해주신 Visual SLAM with Object and Plane의 발표 노트입니다. CubeSLAM의 저자입니다.
---

# 시작하기 전...

## Shichao Yang의 연구 주제

Shape prior를 사용하지 않는 Object와 plane을 이용한 Monocular SLAM.
Large scale로 indoor와 outdoor에서 사용 가능.
Scene understanding이 SLAM에 도움이 되는 것을 증명함.

[CubeSLA](https://www.youtube.com/watch?v=QnVlexXi9_c&ab_channel=ShichaoYang)M의 저자

---

# Point를 사용하는 SLAM의 단점

Point feature를 사용하는 SLAM이 잘 안되는 케이스가 있다.
- 텍스처가 적은 벽이나 바닥
- 움직이는 object가 있을 때
- 회전량이 많을 때
- 다양한 조명 변화

그리고, sparse point map 보다 더 정보가 많이 필요한 task 들도 있다
- 증강현실에서 물리적 상호작용
- 자율주행 중 주변 자동차 위치 파악 (i.e. 정확한 거리 파악)

사람은 그러면 어떻게 localization과 mapping을 잘하는걸까?
일단 Point를 보는건 아닌 것 같다.
Objects와 Planes를 보는 것 같다.
Objects와 plane을 본다는 것은 scene understanding이 있다는 것을 의미한다.

---

# 연구 분야 소개

## 최적화 방법론

1. Decoupled 방법 - Point 기반 SLAM을 수행하고, point cloud로부터 3D object와 plane 검출
- [Bao 2012 CVPR : Semantic structure from motion with points, regions and objects](https://ieeexplore.ieee.org/document/6247992)
- [Dong 2017 CVPR : Visual-inerial-semantic scene representation for 3D object detection](https://openaccess.thecvf.com/content_cvpr_2017/papers/Dong_Visual-Inertial-Semantic_Scene_Representation_CVPR_2017_paper.pdf)
- [Huang 2020 CVPR : ClusterVO - Clustering moving instances and estimating visual odometry](https://arxiv.org/abs/2003.12980)

하나씩 계산하면 되는 쉬운 방법.
하지만 SLAM이 잘 안되거나 Point cloud의 퀄리티가 떨어지면 object detection도 잘 안된다.

2. Tightly coupled 방법 - Camera motion, Object / Plane geometry를 동시에 최적화
- [Salas-Moreno 2012 CVPR : SLAM++ - SLAM at the level of objects](https://ieeexplore.ieee.org/document/6619022)
- [Lee 2017 ICCV : Joint layout estimation and global multi-view registration for indoor reconstruction](https://arxiv.org/abs/1704.07632)
- [Gay 2017 ICCV : Probabilistic Structure from Motion with Objects (PSfMO)](https://openaccess.thecvf.com/content_ICCV_2017/papers/Gay_Probabilistic_Structure_From_ICCV_2017_paper.pdf)
- [Nicholson 2018 RAL : QuadricSLAM - Dual quadrics from object detections as landmarks in object-oriented SLAM](https://arxiv.org/pdf/1804.04011.pdf)
- [Li 2018 ECCV : Stereo vision-based semantic 3D object and ego-motion tracking](https://arxiv.org/abs/1807.02062)

동시에 최적화를 하면서 정확도를 이끌어내려는 시도이다.

<br>

## Object를 표현하는 방법

<br>

{% asset_img "object_representation.png" "Object Representation" %}

## 최적화 cost function

- Object-camera 3D relative pose
  - Object pose estimation을 한 후에, object들의 relative pose를 graph 형태로 표현하는 방법. (e.g. [SLAM++](https://ieeexplore.ieee.org/document/6619022))
  - 구현하기는 쉽지만 3D pose estimation이 굉장히 정확해야함.
    - 그렇기 때문에 depth sensor를 많이 사용하는 편
  
- Object-camera 2D observation
  - 간단하게는 bounding box error나 segmentation error, 복잡한 난이도로는 photometric error나 silhouette error를 사용할 수 있음.

- Object-point 3D position
  - [PnP](https://en.wikipedia.org/wiki/Perspective-n-Point) 방식과 비슷하게, 이미지에서의 2D keypoint와 object의 3D keypoint를 매칭하고 reprojection error를 재는 방법. 

---

# 3D Object detection

- End-to-end CNN
  - Relative pose를 regression으로 풀 수 있는 네트워크들은 대부분 shape prior를 사용함
  - [Li 2018 : DeepIM: Deep Iterative Matching for 6D Pose Estimation](https://arxiv.org/pdf/1804.00175.pdf)

- Deep learning with geometry
  - Shape prior 없이 할 수 있지만, 간단한 object만 가능함 (e.g. 자동차)
  - [Mousavian 2017 : 3D Bounding Box Estimation Using Deep Learning and Geometry](https://arxiv.org/abs/1612.00496)
  - [Chen 2016 : Monocular 3D Object Detection for Autonomous Driving](https://www.cs.toronto.edu/~urtasun/publications/chen_etal_cvpr16.pdf)

<br>

{% asset_img "3d_detection.png" "General approach" %}

하지만 SLAM의 용도로는 우리는 한번도 본 적 없거나 복잡한 object에도 generalise 할 수 있어야한다.

여기 제안하는 새로운 방법은 2D bounding box와 SLAM으로 얻어낸 camera pose를 사용한다.
여러시점의 Camera의 위치로부터 2D bounding box가 projective geometry 기반으로 커버할 수 있는 공간을 계산한다.
그리고 이 공간을 잘 표현하는 cuboid 모델로 표현해주면 간단하게 표현할 수 있다.

<br>

{% asset_img "cuboid.png" "Cuboid model generation" %}

위 사진은 vanishing point를 사용하여 cuboid 모델을 만드는 방법을 소개한다.
Cuboid는 2D 이미지로 보았을 때 3개의 vanishing point가 있어야한다.
굉장히 정확하고 또 간단하게 풀 수 있는 방법이다.
계산된 Cuboid 모델은 항상 2D bounding box 안에서 발견된다.

<br>

{% asset_img "proposals.png" "Cuboid model proposals" %}

위의 방식으로 cuboid 모델을 만들면, 보통 여러개의 가능성이 나온다.
휴리스틱하게 룰을 만들어서 제일 정확한 모델에게 가장 높은 score를 주는 방식을 이전까지는 많이 써왔다.
하지만 이 scoring system을 딥러닝으로 만들 수 있지 않을까 생각해서 도전해봤다.

<br>

{% asset_img "result.png" "Results" %}

실제로 돌려본 결과, object shape prior가 없는데도 cuboid 모양을 잘 잡는 것을 볼 수 있다.
다양한 object들에 cuboid model을 생성할 수 있었다.
Indoor 데이터셋에서는 굉장히 잘 되었다.
Outdoor 데이터셋은 [KITTI 데이터셋](http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d)을 사용했는데, 우리의 방식보다 결과가 좋게 나왔다.
그도 그럴만한게, KITTI 데이터셋에는 shape prior + 다양한 차들의 데이터가 충분히 있다.
Shape prior가 없는 우리 방식의 성능을 뛰어넘을 수 있다는 것을 인지해야한다.

---

# Plane detection

- Model-based 방식
  - Cuboid 형태의 방만 가능
  - [Hedau 2009 : Recovering the Spatial Layout of Cluttered Rooms](http://dhoiem.web.engr.illinois.edu/publications/iccv2009_hedau_indoor.pdf)
  - [Schwing 2013 : Box in the Box: Joint 3D Layout and Object Reasoning from Single Images](https://ieeexplore.ieee.org/document/6751153)

이런 방식은 복도와 같이 반대편 벽이 보이지 않는 경우에는 사용할 수 없다.

- Learning-based 방식
  - [Zou 2018 : LayoutNet: Reconstructing the 3D Room Layout rom a Single RGB Image](https://openaccess.thecvf.com/content_cvpr_2018/papers/Zou_LayoutNet_Reconstructing_the_CVPR_2018_paper.pdf) 
  - [Liu 2018 : PlaneNet: Piece-wise Planar Reconstruction from a Single RGB Image](https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_PlaneNet_Piece-Wise_Planar_CVPR_2018_paper.pdf)

딥러닝 기반 layout detection은 매 프레임마다 layout prediction을 하기 때문에 정확한 값을 얻을 수 없다. (i.e. layout이 떨린다)

여기에 SLAM을 섞으면 어떨까?
SLAM의 경우, 하나의 feature point를 다양한 각도에서 봐도 안정적이게 추출할 수 있다.
이 점을 이용하면, layout도 여러 각도에서 봐도 안정적이게 추출할 수 있을 것이다.

<br>

{% asset_img "plane_proposal.png" "Plane proposal" %}

우선, 기존의 line detector를 써서 line을 찾는다.
Occlusion된 line이 있을 수 있으니, segmentation을 사용해서 object를 찾고 segmentation mask 내부에 line을 더 이어서 그려준다.

<br>

{% asset_img "plane_proposal_2.png" "Plane proposal 2" %}

위의 방법으로 여러개의 plane candidate가 나올 것이다.
벽과 바닥이 90도 직각이라는 가정을 두고, wall plane과 floor plane을 뽑아준다.
이 때, 3D object detection도 할 것이라면, 동시에 같이 뽑을 수도 있다.

<br>

{% asset_img "object_plane_joint.png" "Object / Plane joint optimisation" %}

3D object pose와 wall/floor plane 정보를 함께 이용해서 joint optimisation을 할 수 있다.
예를 들어, object의 기울기는 floor plane과 평행해야할 수도 있다.
또, object의 cuboid는 wall/floor plane과 intersect 할 수 없다. (i.e. intersect하면 벽을 뚫은거니까...)
이러한 추가적인 geometric constraint를 추가해줌으로써 더욱 정확하게 결과를 뽑을 수 있다.

---

# Monocular SLAM with objects and planes

<br>

{% asset_img "representations.png" "Objects and planes as factors" %}

기존의 SLAM에서는 factor graph optimisation을 할 때 camera pose와 map point location만 고려한다.
여기에 object와 plane 정보도 함께 넣어 최적화 하는것이다.
즉, tightly-coupled optimisation을 수행하는 것이다.

Objects와 Planes는 어떤 방식으로 factor로 표현할 수 있을까?
Plane의 경우, 3 dof를 가지는 $\pi = [n\ d]$ 가 되겠다.
Cuboid object의 경우, 6 dof pose와 (i.e. 3d rotation + 3d translation) 3 dof size를 (i.e. width, length, height) 가지겠다.
종종 특정 용도에서 자동차의 경우 size를 고정하기도 한다.

<br>

{% asset_img "measurement_error.png" "Measurement errors" %}

최적화를 수행할 때는 measurement error 값을 이용한다.
여기서 우리는 두가지 방식으로 measurement error를 사용할 수 있다.

첫번째는 3D measurement이다. 
다양한 카메라 각도에서 object의 pose와 size를 추정하면서 나타나는 차이 값이다.

두번째는 2D measurement이다.
Cuboid를 2D 이미지에 projection시켜 생기는 사각형을 2D bounding box와 비교하는 reprojection error를 사용하는 것이다.
이 방법은 3D measurement를 사용하는 방법보다 더 robust하다.

<br>

{% asset_img "measurement_error2.png" "Measurement errors 2" %}

Plane 정보는 항상 바닥에서 추출한 line으로부터 만들어진다.
그렇기 때문에, 우리가 추정한 plane 정보는 unproject된 바닥 line과 비교하여 에러 값을 추정할 수 있다.

<br>

{% asset_img "data_association.png" "Data association" %}

위의 방법들을 사용하면, 우리는 자연스럽게 point feature와 object 정보에 대해서 data association을 할 수 있다.
여기에 epipolar checking 등을 하면 더 정확해질 수 있다.
이 방법으로 cluttered / repetitive / occluded object에 대해서도 강인하게 트랙킹하는 것을 보았다.
이 방법은 기존의 object tracking이나 template 기반 트랙킹 방법보다 정확하다.

<br>

{% asset_img "eval.png" "Evaluation" %}

Object의 크기를 fixed-size로 고정해놓고 이 방법을 구현한 것이 CubeSLAM이다.
CubeSLAM은 loop closure과 같은 기능도 없지만, scale drift가 전혀 없는 것을 볼 수 있다.
ORB-SLAM과 비교하였을 때 훨씬 정확한 모습을 볼 수 있다.

<br>

{% asset_img "eval2.png" "Evaluation 2" %}

이런 경우에는 feature point의 수가 굉장히 적기 때문에 기존의 ORB-SLAM과 같은 방식은 모두 실패한다.


<br>

{% asset_img "eval3.png" "Evaluation 3 %}

이번 경우에서는 object의 pose가 많이 흔들린 것을 볼 수 있다.
이는 wall 정보를 사용하지 않았기 때문이라고 판단하여, 후속 연구에서 wall 정보를 사용하는 것을 고려하여 만들었다.


